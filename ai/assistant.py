"""
Bu dosya, filtrelenmiÅŸ ERP verisini (DataFrame) baÄŸlam olarak kullanarak
Ollama Ã¼zerindeki LLM'e soru yÃ¶nlendirir, cevabÄ± Streamlit arayÃ¼zÃ¼nde
token token (streaming) ÅŸekilde gÃ¶sterir ve modeli tabloya gÃ¶re
cevap vermeye zorlar.
"""

import pandas as pd
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.callbacks import BaseCallbackHandler

from typing import Any

class StreamlitHandler(BaseCallbackHandler):
    def __init__(self, placeholder):
        self.placeholder = placeholder
        self.final_text = ""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        self.final_text += token

        bubble_html = f"""
        <div class="chat-container">
            <div class="bubble bot-bubble">
                ğŸ” {self.final_text}
            </div>
        </div>
        """

        self.placeholder.markdown(bubble_html, unsafe_allow_html=True)

        
def run_ai(prompt: str, subset_df, chat_history, placeholder):

    clean_df = subset_df.copy()
    context_table = clean_df.drop(columns=['xml_ubl'], errors='ignore').head(15).to_string(index=False)

    if clean_df.empty:
        placeholder.markdown("ÃœzgÃ¼nÃ¼m, filtrelediÄŸiniz kriterlere uygun fatura bulunamadÄ±.")
        return "ÃœzgÃ¼nÃ¼m, filtrelediÄŸiniz kriterlere uygun fatura bulunamadÄ±."

    system_content = f"Sen bir ERP uzmanÄ±sÄ±n. Tabloya gÃ¶re cevap ver.\n\nTablo:\n{context_table}"

    messages = [SystemMessage(content=system_content)] + chat_history + [HumanMessage(content=prompt)]

    stream_handler = StreamlitHandler(placeholder)

    llm = ChatOllama(
        model="llama3.2:3b",
        temperature=0,
        streaming=True,
        callbacks=[stream_handler]
    )

    response = llm.invoke(messages)

    return response.content

"""
Bu dosya, filtrelenmiÅŸ ERP verisini (DataFrame) baÄŸlam olarak kullanarak
Ollama Ã¼zerindeki LLM'e soru yÃ¶nlendirir, cevabÄ± Streamlit arayÃ¼zÃ¼nde
token token (streaming) ÅŸekilde gÃ¶sterir ve modeli tabloya gÃ¶re
cevap vermeye zorlar.
"""

import pandas as pd
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.callbacks import BaseCallbackHandler

from typing import Any

class StreamlitHandler(BaseCallbackHandler):
    def __init__(self, placeholder):
        self.placeholder = placeholder
        self.final_text = ""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        self.final_text += token

        bubble_html = f"""
        <div class="chat-container">
            <div class="bubble bot-bubble">
                ğŸ” {self.final_text}
            </div>
        </div>
        """

        self.placeholder.markdown(bubble_html, unsafe_allow_html=True)

        
# assistant.py (Ä°lgili kÄ±sÄ±m)
def run_ai(prompt: str, subset_df, chat_history, placeholder):
    # Ollama'yÄ± hazÄ±rla
    stream_handler = StreamlitHandler(placeholder)
    llm = ChatOllama(model="llama3.2:3b", temperature=0, streaming=True, callbacks=[stream_handler])

    # YÃ¶nlendiriciyi Ã§alÄ±ÅŸtÄ±r
    result = route_question(prompt, chat_history, llm)

    # EÄŸer Vanna'dan veri geldiyse sonucu kullanÄ±cÄ±ya aÃ§Ä±kla
    if "VERI_SONUCU:" in str(result):
        # Burada sonucu tekrar Ollama'ya sorup kibar bir dille aÃ§Ä±klatÄ±yoruz
        explanation = llm.invoke(f"Bu veritabanÄ± sonucunu kullanÄ±cÄ±ya Ã¶zetle: {result}")
        return explanation.content

    return str(result)
